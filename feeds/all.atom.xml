<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>panacea.icu</title><link href="https://panacea.icu/" rel="alternate"></link><link href="https://panacea.icu/feeds/all.atom.xml" rel="self"></link><id>https://panacea.icu/</id><updated>2021-02-11T13:32:00+08:00</updated><entry><title>强化学习</title><link href="https://panacea.icu/qiang-hua-xue-xi.html" rel="alternate"></link><published>2021-02-11T13:32:00+08:00</published><updated>2021-02-11T13:32:00+08:00</updated><author><name>Pink</name></author><id>tag:panacea.icu,2021-02-11:/qiang-hua-xue-xi.html</id><summary type="html">&lt;p&gt;最近看了一下 &lt;em&gt;Reinforcement Learning:
An Introduction&lt;/em&gt; 的前两部分。剩下的由于我耐心耗尽，应该现在不会看，等耐心条恢复差不多了大概再看一下后面的案例和前沿研究两章。暂时阶段性总结一下。这书貌似看的人还挺多的，不过我感觉写得大概也就算&lt;em&gt;中规中矩&lt;/em&gt;，没感觉有多好。当然，貌似也没听说什么更好的，只能看这个。这本书作者按 CC BY-NC-ND 协议分享，所以可以直接下载电子版而没有任何版权问题。&lt;/p&gt;
&lt;p&gt;我对机器学习的兴趣一般，这东西应该很难达到一些人所许诺的和很多人所期许的效果。但这方面确实也有一些不错的成果，比如之前很多人说的蛋白质折叠。总之，应该正常看待。至于强化学习，我感觉要在 real world 里（而不是人为设计的游戏和任务里）真正很好地用起来也并不容易。不过，曾经有个人和我说他用这东西炒股，并且还赚到钱了。。。（有可能确实是真的，但这我无法确认。）最近正好我有点这方面需要，就看着书学一下也挺好的。&lt;/p&gt;
&lt;p&gt;强化学习这学科应该完全属于实践的学科，关键在于怎么用，具体理论我感觉重要程度也就一般。这书里面经常说，A 方法理论上比较好，B 方法实践中比较好 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;最近看了一下 &lt;em&gt;Reinforcement Learning:
An Introduction&lt;/em&gt; 的前两部分。剩下的由于我耐心耗尽，应该现在不会看，等耐心条恢复差不多了大概再看一下后面的案例和前沿研究两章。暂时阶段性总结一下。这书貌似看的人还挺多的，不过我感觉写得大概也就算&lt;em&gt;中规中矩&lt;/em&gt;，没感觉有多好。当然，貌似也没听说什么更好的，只能看这个。这本书作者按 CC BY-NC-ND 协议分享，所以可以直接下载电子版而没有任何版权问题。&lt;/p&gt;
&lt;p&gt;我对机器学习的兴趣一般，这东西应该很难达到一些人所许诺的和很多人所期许的效果。但这方面确实也有一些不错的成果，比如之前很多人说的蛋白质折叠。总之，应该正常看待。至于强化学习，我感觉要在 real world 里（而不是人为设计的游戏和任务里）真正很好地用起来也并不容易。不过，曾经有个人和我说他用这东西炒股，并且还赚到钱了。。。（有可能确实是真的，但这我无法确认。）最近正好我有点这方面需要，就看着书学一下也挺好的。&lt;/p&gt;
&lt;p&gt;强化学习这学科应该完全属于实践的学科，关键在于怎么用，具体理论我感觉重要程度也就一般。这书里面经常说，A 方法理论上比较好，B 方法实践中比较好。。。。他书里写的几个估计出来的误差的上界都巨大无比，实际中用处也不大。。所以有关收敛的、误差的这种定理的证明我也懒得管了，很多他书里也没写。强烈怀疑就是不动点定理之类的。&lt;/p&gt;
&lt;p&gt;按我理解，这个最大的贡献，就是提出了 state/action/reward 不停循环的这一个统一的模型。这一模型的适用性十分广泛，这样可以把很多问题放进他这个体系进行分析和处理。&lt;/p&gt;
&lt;p&gt;在这之后，另一个重要的概念就是效用函数 &lt;span class="math"&gt;\(v_\pi: S \rightarrow \mathbb R\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(q_\pi: S \times A \rightarrow \mathbb R\)&lt;/span&gt; ，表示在某个状态 &lt;span class="math"&gt;\(s\)&lt;/span&gt; 或某个 &lt;span class="math"&gt;\((s, a)\)&lt;/span&gt; 组合下按策略 &lt;span class="math"&gt;\(\pi\)&lt;/span&gt; 未来的总预期收益。这地方有一个比较显然的关系，就是向后递推一步的关系，被称为 Bellman 方程。&lt;span class="math"&gt;\(v_\pi\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(q_\pi\)&lt;/span&gt; 的关系在于，如果知道系统的转移概率这两者可以互相推，由于前者的计算量比后者小，因此前者比较有用；而如果不知道系统怎么转移，只能从 &lt;span class="math"&gt;\(q_\pi\)&lt;/span&gt; 推 &lt;span class="math"&gt;\(v_\pi\)&lt;/span&gt; ，而不能相反，因此只能用后者。&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(v_\pi\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(q_\pi\)&lt;/span&gt; 有两种不同的用法。一种是利用这两个函数&lt;strong&gt;改进&lt;/strong&gt;策略，一种是用来&lt;strong&gt;衡量&lt;/strong&gt;策略的好坏，分别对应于。前者叫 &lt;em&gt;Generalized Policy Iteration&lt;/em&gt; ，后者叫 &lt;em&gt;Policy Gradient Methods&lt;/em&gt;。Generalized Policy Iteration 的意思是，对于每个 &lt;span class="math"&gt;\(s\)&lt;/span&gt; ，根据 &lt;span class="math"&gt;\(q_\pi\)&lt;/span&gt; 可以看出选哪个最好，这样直接选最大的就 OK 了。此处在一些情况下（例如打表），利用 &lt;em&gt;Policy Improvement Theorem&lt;/em&gt; 可以证明策略确实在提升，而更多的时候即使无法证明，在实际中也是有效的。策略改进后再重新评估 &lt;span class="math"&gt;\(v_{\pi'}\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(q_{\pi'}\)&lt;/span&gt; ，无限循环就可以了。而 Policy Gradient Methods 的意思是，直接通过一些参数控制策略，通过这些量衡量策略整体的好坏，通过改变策略的参数优化策略的表现。&lt;/p&gt;
&lt;p&gt;这本书 Generalized Policy Iteration 介绍得十分详细，而 Policy Gradient Methods 只有一章，极简略地说了一下。。。。实属有点问题。。。而 Policy Gradient Methods 相比 Generalized Policy Iteration 在某些方面是有优势的。例如，可以不停训练让 soft 的策略收敛到 deterministic 的策略，可以提供已知的基础性的策略等等。这地方应该根据后面的 Bibliographical and Historical Remarks 或者其他资料再学习一下。（不过，我耐心已耗尽，以后再说。。）&lt;/p&gt;
&lt;p&gt;对于 Generalized Policy Iteration ，其实就是如下问题&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;如何计算 &lt;span class="math"&gt;\(v_\pi\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(q_\pi\)&lt;/span&gt;。大概有如下的办法：(1) 直接按定义强行试，就是 Monte Carlo 方法。(2) 利用 Bellman 方程，进行 &lt;em&gt;bootstrap&lt;/em&gt; ，即相当于是提供 &lt;span class="math"&gt;\(n\)&lt;/span&gt; 步的信息，剩下的用目前的估计补上。这样每一步提供一些信息改进一点点，最后慢慢就可以了&lt;/li&gt;
&lt;li&gt;然而，Bellman 方程需要环境的转移概率，这在很多时候是不知道的。当然，这无所谓，因为反正本质是期望，所以还是不停试就完事了&lt;/li&gt;
&lt;li&gt;所以，如果环境转移概率未知，不管用什么办法，最后肯定都得靠试，这是肯定的。然而，这有个问题，就是如果用一个 deterministic 的策略的话，那每次选的都是固定的一个，很多地方就都试不到了。试不到的地方就估计不到。此处有两个办法，分别为 on-policy 和 off-policy 。如果是 on-policy ，大概意思就是说，把策略的空间限制小，要求是 &lt;span class="math"&gt;\(\varepsilon\)&lt;/span&gt;-soft 的，即每个策略至少有 &lt;span class="math"&gt;\(\varepsilon\)&lt;/span&gt; 的概率被取到。这样显然是可以的，一般也比较方便，不过得出的仅仅是在这一空间内的最优策略，而不是整体的最优策略。Off-policy 的意思是，用另一套 soft 的策略去试，然后去优化要优化的策略。当然，这样对另一套策略试出的的结果肯定要进行修正，能得到最优策略但过程比较费劲。最典型的例子就是 Q-learning&lt;/li&gt;
&lt;li&gt;以上只是说了可以算，问题在于如果 &lt;span class="math"&gt;\(S\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(A\)&lt;/span&gt; 都很大或者干脆无限，根本就算不完，算完了内存也存不下。解决的办法就是不要打表了，给定一个函数的空间 &lt;span class="math"&gt;\(\mathbb R^n\rightarrow \mathrm{Hom}(S, \mathbb R)\)&lt;/span&gt;，从这里面选一个参数 &lt;span class="math"&gt;\(\mathbf{w}\)&lt;/span&gt; 出来使得对应的函数最接近 &lt;span class="math"&gt;\(v_\pi\)&lt;/span&gt; 就完事了。（ &lt;span class="math"&gt;\(q_\pi\)&lt;/span&gt; 同理。）此处可以利用 supervised learning 的东西。几乎全是奇技淫巧。当然，这是有意义的，因为靠打表存函数势必浪费很多信息，确实会存在更高效的方法。至少按照 Fourier 变换的理论，连续性条件可以保证绝大部分信息存在于频谱的中心。至于怎么选出 &lt;span class="math"&gt;\(\mathbf{w}\)&lt;/span&gt; ，显然还是梯度下降。。。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;顺便关于 Eligibility Trace 说一句，这东西其实是很自然的。因为对于 n-step TD，按照书里的说法，每一步算 &lt;span class="math"&gt;\(G_{t:t+n}\)&lt;/span&gt; 每一步都直接按求和算，这样，相比 TD(0) 和 Monte Carlo，每一步的开销岂不是提高了 &lt;span class="math"&gt;\(n\)&lt;/span&gt; 倍。当然，这也可以通过比较这加加减减算，但就比较麻烦。所以，就发现这东西实际不应该这么加加减减，应该通过乘一个 factor &lt;span class="math"&gt;\(\gamma &amp;lt; 1\)&lt;/span&gt; 让它逐渐消失。这样就是 Eligibility Trace ，算法写出来就很漂亮。（不过我感觉在效果上也就那么回事。。。）&lt;/p&gt;
&lt;p&gt;此外，有一个问题在于，很多东西在时间上是连续的，书中也有这种例子。这东西问题在于，如果强行直接用 &lt;span class="math"&gt;\(+\Delta t \cdot y'(t)\)&lt;/span&gt; 的方法（"Explicit Euler"），要不误差就很大，要不 &lt;span class="math"&gt;\(\Delta t\)&lt;/span&gt; 就很小，非常慢，这就十分麻烦。我考虑的办法是，只生成有限长的 episode ，不 online 学习。先把时间等分，每次第一次落入某个时间间隔的时候按策略选择 action ，然后存起来，这样后续再进来的时候直接读，确保前后是一致的。这样就可以用常见的方法积分（比如 Runge-Kutta），积分后统一处理就可以了。只要提供了新信息并且 Bellman 方程加了 &lt;span class="math"&gt;\(\mathbb E\)&lt;/span&gt; 之后是对的，我怀疑应该怎么更新都行。。。此外，按我查的结果，关于这种连续的情况，有比较新的 paper 在讨论，不过看懂这东西目前在我能力范围以外。。。。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="阅读"></category></entry><entry><title>关于评论</title><link href="https://panacea.icu/guan-yu-ping-lun.html" rel="alternate"></link><published>2021-02-09T09:11:00+08:00</published><updated>2021-02-09T09:11:00+08:00</updated><author><name>Pink</name></author><id>tag:panacea.icu,2021-02-09:/guan-yu-ping-lun.html</id><content type="html">&lt;p&gt;本网站使用了 &lt;a href="https://disqus.com/"&gt;DISQUS&lt;/a&gt; 作为评论系统。
然而，&lt;a href="https://disqus.com/"&gt;DISQUS&lt;/a&gt; 在中华人民共和国（大陆）、朝鲜民主主义人民共和国等国家和地区内可能无法正确加载。
我正在寻求替代方案，但在此说明一下这一问题并不是本网站 bug 所致。&lt;/p&gt;
&lt;p&gt;此外，严禁在评论区发表违反中华人民共和国各项法律和规章制度、违背社会主义核心价值观的任何内容。
一经发现，立即删除！&lt;/p&gt;
&lt;p&gt;Any comment against the laws and regulations of the PRC is strictly prohibited and will be deleted immediately. &lt;/p&gt;</content><category term="网站说明"></category></entry><entry><title>内容协议</title><link href="https://panacea.icu/nei-rong-xie-yi.html" rel="alternate"></link><published>2021-02-09T08:51:00+08:00</published><updated>2021-02-09T08:51:00+08:00</updated><author><name>Pink</name></author><id>tag:panacea.icu,2021-02-09:/nei-rong-xie-yi.html</id><content type="html">&lt;p&gt;如未特殊说明，本网站的所有文章和内容应在
&lt;em&gt;Creative Commons Attribution-NonCommercial 4.0 International License&lt;/em&gt;
协议许可的条件下使用。&lt;/p&gt;
&lt;p&gt;To view a copy of this license, click &lt;a href="https://creativecommons.org/licenses/by-nc/4.0/"&gt;here&lt;/a&gt;.&lt;/p&gt;</content><category term="网站说明"></category></entry></feed>